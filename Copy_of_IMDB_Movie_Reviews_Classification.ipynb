{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of IMDB Movie Reviews Classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Maximluking/AndroidWeather/blob/master/Copy_of_IMDB_Movie_Reviews_Classification.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "AfiM5IWolB2_",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Загружаем модули**"
      ]
    },
    {
      "metadata": {
        "id": "EFzZKjT-BAI-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "0e8a7cf9-2650-4c3a-d9f5-d82fa51112cd"
      },
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import models\n",
        "from keras import layers\n",
        "\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "'''\n",
        "\n",
        "https://github.com/keras-team\n",
        "\n",
        "можно посчитать колличество слов в позитивных и негативнх рецензиях. скорее всего положительные длиннее\n",
        "\n",
        "вывести сколько положительных и отрицательных отзывов в тренировочной и тестовой выборке\n",
        "интересен баланс\n",
        "ощущение что негативных намного больше\n",
        "\n",
        "факты о базе данных\n",
        "\n",
        "Dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). \n",
        "Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \n",
        "For convenience, words are indexed by overall frequency in the dataset, \n",
        "so that for instance the integer \"3\" encodes the 3rd most frequent word in the data.\n",
        "This allows for quick filtering operations such as: \"only consider the top 10,000 most common words,\n",
        "but eliminate the top 20 most common words\".\n",
        "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n\\nhttps://github.com/keras-team\\n\\nможно посчитать колличество слов в позитивных и негативнх рецензиях. скорее всего положительные длиннее\\n\\nвывести сколько положительных и отрицательных отзывов в тренировочной и тестовой выборке\\nинтересен баланс\\nощущение что негативных намного больше\\n\\nфакты о базе данных\\n\\nDataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). \\nReviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). \\nFor convenience, words are indexed by overall frequency in the dataset, \\nso that for instance the integer \"3\" encodes the 3rd most frequent word in the data.\\nThis allows for quick filtering operations such as: \"only consider the top 10,000 most common words,\\nbut eliminate the top 20 most common words\".\\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "bDFnTFUOHDgF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qTFBA00rHD3T",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Q0d4KlT6HEqa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o6cPFGKAlZxX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# edu_utils"
      ]
    },
    {
      "metadata": {
        "id": "bezlArGzS2-k",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "# прекращаем закодированные отзывы из цифр в текст для человека\n",
        "def decode_review(word_index, review):\n",
        "  reverse_word_index = dict(\n",
        "  [(value, key) for (key, value) in word_index.items()])\n",
        "  decoded_review = ' '.join(\n",
        "  [reverse_word_index.get(i - 3, '?') for i in review])\n",
        "  return decoded_review\n",
        "\n",
        "# выводим отзыв для просмотра\n",
        "def print_review(review):\n",
        "  review_length = len(review)\n",
        "  line_length = 150\n",
        "  lines_number = math.ceil(review_length / float(line_length))\n",
        "  for i in range(lines_number):\n",
        "    if line_length > review_length:\n",
        "      print(review)\n",
        "    else:\n",
        "      left_string_chank_len = review_length - i * line_length\n",
        "      if left_string_chank_len < line_length:\n",
        "        print(review[i * line_length:])\n",
        "      else:\n",
        "        print(review[i * line_length: (i + 1) * line_length])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oYepAy1Gmf3G",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Загружаем данные, делим их на валдицаию и тест, смотрим**"
      ]
    },
    {
      "metadata": {
        "id": "noWGAiKYB9M7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "c06d89ec-c742-4829-ead5-0e0a6f04302b"
      },
      "cell_type": "code",
      "source": [
        "#1.Загружаем данные\n",
        "unique_words = 10000\n",
        "(train_data_imdb, train_labels_imdb), (test_data_imdb, test_labels_imdb) = imdb.load_data(num_words=unique_words)\n",
        "\n",
        "validation_review_number = 1000\n",
        "test_review_number = 1000\n",
        "\n",
        "test_data   = test_data_imdb[:test_review_number]\n",
        "test_labels = test_labels_imdb[:test_review_number] \n",
        "\n",
        "validation_data   = test_data_imdb[test_review_number : validation_review_number + test_review_number]\n",
        "validation_labels = test_labels_imdb[test_review_number : validation_review_number + test_review_number]\n",
        "\n",
        "train_data   = np.concatenate((train_data_imdb, test_data_imdb[validation_review_number + test_review_number:]), axis=0)\n",
        "train_labels = np.concatenate((train_labels_imdb, test_labels_imdb[validation_review_number + test_review_number:]), axis=0)\n",
        "\n",
        "# Загружаем словарь \"из фир в слова\"\n",
        "word_index = imdb.get_word_index()\n",
        "\n",
        "print('Колличество тренировачных обзоров: \\n', train_data.shape[0],\n",
        "      '\\nКолличество валидационных обзоров: \\n', validation_data.shape[0],\n",
        "      '\\nКолличество тестовых обзоров: \\n', test_data.shape[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 2s 0us/step\n",
            "Downloading data from https://s3.amazonaws.com/text-datasets/imdb_word_index.json\n",
            "1646592/1641221 [==============================] - 1s 0us/step\n",
            "Колличество тренировачных обзоров: \n",
            " 48000 \n",
            "Колличество валидационных обзоров: \n",
            " 1000 \n",
            "Колличество тестовых обзоров: \n",
            " 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PeIMjl4xe4h7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "5b2fc0fe-8f18-4764-cde3-3cbc933ca9bf"
      },
      "cell_type": "code",
      "source": [
        "# посмотреть один отзыв\n",
        "review = decode_review(word_index, test_data[4])\n",
        "print_review(review)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "? like some other people wrote i'm a die hard mario fan and i loved this game br br this game starts slightly boring but trust me it's worth it as soo\n",
            "n as you start your hooked the levels are fun and ? they will hook you ? your mind turns to ? i'm not kidding this game is also ? and is beautifully d\n",
            "one br br to keep this spoiler free i have to keep my mouth shut about details but please try this game it'll be worth it br br story 9 9 action 10 1 \n",
            "it's that good ? 10 attention ? 10 average 10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "NkyA9aOqtFKO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Обработка данных в нужнуый для сети формат**"
      ]
    },
    {
      "metadata": {
        "id": "EDB6eRKWSuPT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "b20f859e-ac0b-49a8-b9cd-a75853d87072"
      },
      "cell_type": "code",
      "source": [
        "#3.preprocess\n",
        "tokenizer = Tokenizer(num_words=unique_words)\n",
        "train_data_binary = tokenizer.sequences_to_matrix(train_data, mode='binary')\n",
        "test_data_binary = tokenizer.sequences_to_matrix(test_data, mode='binary')\n",
        "validation_data_binary = tokenizer.sequences_to_matrix(validation_data, mode='binary')\n",
        "\n",
        "print('\\nИсходный вид отзыва в базе данных: \\n', train_data[1000],\n",
        "      '\\nОтзыв для человека: \\n', review,\n",
        "      '\\nКак отзыв выглядит для нейронной сети: \\n', train_data_binary)\n",
        "\n",
        "print(train_data_binary.shape)\n",
        "print(test_data_binary.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Исходный вид отзыва в базе данных: \n",
            " [1, 261, 13, 69, 110, 2, 11, 6, 750, 96, 145, 11, 2, 13, 426, 377, 233, 7, 4, 114, 549, 18, 3482, 1218, 7, 3199, 2102, 620, 5, 997, 429, 6, 4157, 7, 1654, 3404, 5, 1387, 2795, 4, 277, 10, 10, 103, 886, 49, 7, 4, 85, 857, 13, 1637, 56, 6, 1039, 7, 4, 4700, 626, 288, 8, 280, 174, 2541, 4, 182, 7, 2, 10, 10, 12, 505, 46, 14, 9, 31, 7, 148, 108, 1055, 315, 4, 7423, 15, 62, 140, 2555, 8, 374, 639, 4, 22, 381, 5810, 2, 3199, 2102, 17, 4409, 2, 2, 46, 7, 4, 1336, 8, 2, 4, 2952, 7, 2, 8, 2541, 5, 2363, 1176, 4, 500, 6, 2, 2, 2, 34, 4, 4698, 37, 9090, 27, 84, 34, 4578, 51, 934, 40, 2, 671, 4, 3359, 7, 4, 1973, 2290, 4, 323, 1650, 4824, 1510, 4409, 9, 4415, 11, 4, 1647, 1733, 34, 2549, 2, 37, 115, 2466, 42, 889, 4, 313, 280, 4415, 497, 8, 3890, 11, 19, 4, 5751, 34, 1658, 6, 1927, 767, 2, 19, 2, 7, 1336, 5, 428, 2964, 8, 135, 2, 659, 309, 620, 5, 997, 18, 27, 113, 367, 4, 1654, 1986, 7, 2, 23, 27, 96, 8, 6, 1304, 19, 4205, 5, 4, 500, 10, 10, 323, 3199, 2102, 16, 1815, 3009, 34, 167, 670, 2, 37, 12, 272, 40, 16, 115, 23, 4, 270, 4, 38, 446, 229, 9, 43, 14, 499, 7, 4767, 67, 85, 857, 18, 4, 111, 2, 469, 4, 513, 7, 2, 47, 6, 171, 52, 388, 21, 9, 1116, 2262, 34, 78, 802, 4, 277, 2, 131, 225, 4, 2, 7, 6, 52, 206, 1154, 133, 6, 2579, 1991, 310, 19, 53, 8904, 206, 5, 4291, 1062, 238, 60, 30, 184, 52] \n",
            "Отзыв для человека: \n",
            " ? like some other people wrote i'm a die hard mario fan and i loved this game br br this game starts slightly boring but trust me it's worth it as soon as you start your hooked the levels are fun and ? they will hook you ? your mind turns to ? i'm not kidding this game is also ? and is beautifully done br br to keep this spoiler free i have to keep my mouth shut about details but please try this game it'll be worth it br br story 9 9 action 10 1 it's that good ? 10 attention ? 10 average 10 \n",
            "Как отзыв выглядит для нейронной сети: \n",
            " [[0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 1. ... 0. 0. 0.]\n",
            " [0. 1. 1. ... 0. 0. 0.]]\n",
            "(48000, 10000)\n",
            "(1000, 10000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2KqBLeC59YLJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Строим модель**"
      ]
    },
    {
      "metadata": {
        "id": "JQT-HJXPW9Nu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "f71689cd-0192-4378-9152-7779fbd0bcc0"
      },
      "cell_type": "code",
      "source": [
        "#the model\n",
        "model = models.Sequential()\n",
        "model.add(layers.Dense(10, activation='relu', input_shape=(unique_words,)))\n",
        "model.add(layers.Dropout(0.4))\n",
        "model.add(layers.Dense(10, activation='relu'))\n",
        "model.add(layers.Dropout(0.1))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_1 (Dense)              (None, 10)                100010    \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 10)                110       \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 11        \n",
            "=================================================================\n",
            "Total params: 100,131\n",
            "Trainable params: 100,131\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5PjnqYgD9eQl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Устанавливаем оптимизатор, функицю потерь, метрику**"
      ]
    },
    {
      "metadata": {
        "id": "AmBXMl49XBlf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='rmsprop',\n",
        "loss='binary_crossentropy',\n",
        "metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UFFBpAvj9oz6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Обучаем на тренировочны и проверяем на тестов данных**"
      ]
    },
    {
      "metadata": {
        "id": "0PirLZvwbq8f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "ad728fd6-e2e6-4c25-cd07-f232c0983b59"
      },
      "cell_type": "code",
      "source": [
        "print(train_labels.shape)\n",
        "history = model.fit(train_data_binary, train_labels,\n",
        "                    epochs=5,\n",
        "                    batch_size=512,\n",
        "                    validation_data=(validation_data_binary, validation_labels))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(48000,)\n",
            "Train on 48000 samples, validate on 1000 samples\n",
            "Epoch 1/5\n",
            "48000/48000 [==============================] - 6s 130us/step - loss: 0.5057 - acc: 0.7726 - val_loss: 0.3544 - val_acc: 0.8840\n",
            "Epoch 2/5\n",
            "48000/48000 [==============================] - 4s 90us/step - loss: 0.3458 - acc: 0.8702 - val_loss: 0.2730 - val_acc: 0.8990\n",
            "Epoch 3/5\n",
            "48000/48000 [==============================] - 5s 97us/step - loss: 0.2788 - acc: 0.8999 - val_loss: 0.2462 - val_acc: 0.9040\n",
            "Epoch 4/5\n",
            "48000/48000 [==============================] - 5s 98us/step - loss: 0.2427 - acc: 0.9136 - val_loss: 0.2378 - val_acc: 0.9050\n",
            "Epoch 5/5\n",
            "48000/48000 [==============================] - 5s 97us/step - loss: 0.2177 - acc: 0.9229 - val_loss: 0.2399 - val_acc: 0.9040\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "M7QvvehnHhnd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Тестируем **"
      ]
    },
    {
      "metadata": {
        "id": "2X9jDSzSPrqM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "foPyOVAVcGQq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3e8c50aa-a88c-473e-c42c-a89891f8863d"
      },
      "cell_type": "code",
      "source": [
        "test_review = (test_data_binary[100:101])\n",
        "y = model.predict(test_review)\n",
        "print_review(test_review)\n",
        "print('answer: ', y)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0. 1. 1. ... 0. 0. 0.]]\n",
            "answer:  [[0.08259585]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g_tnROl_F1yc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# **Перевод рецензий**"
      ]
    },
    {
      "metadata": {
        "id": "5JCtsATcF7pu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "негатив 1. test_data[1000]"
      ]
    },
    {
      "metadata": {
        "id": "mcAzxSjLGGPU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "?? Это фильм был одним из худших, которые я когда либо видел ? Я смотрел его с моей девушкой на предыдущих выходных и продолжали его просмотр только лишь в надежде, что он станет лучше, но нет.\n",
        "Качество самой картинки ужасное, такое ощущение , чтое его сняли на мобилку, плюс плохой свет не дает нормально проникнуться емоцияями на лицах актеров. Впрочем и сама игра актеров никуда не годится, если не брать во внимание, что это очередной тинейджерский ужастик. Звук был реальной проблемой: иногда приходилось ? звук в видео, потому что он был нечетким и не выдерживает никакой критики. Лучше не тратить на этот ? фильм своего времени.\n"
      ]
    },
    {
      "metadata": {
        "id": "Do4y9JiDGETX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? ? this is one of the worst movies i have ever seen the ? for the film is better than the film itself my girlfriend and i watched it this past weeken\n",
        "d and we only continued to watch it in the hopes that it would get better it didn't br br the picture quality is poor it looks like it was shot on vid\n",
        "eo and transferred to film the lighting is not great which makes it harder to read the actors' facial expressions the acting itself was cheesy but i g\n",
        "uess it's acceptable for yet another teenage horror flick the sound was a huge problem sometimes you have to ? the video because the sound is unclear \n",
        "and or ? br br it holds no real merit of it's own trying to ride on the ? of sleepy hollow don't bother with this one"
      ]
    },
    {
      "metadata": {
        "id": "Vdxl261GIj0f",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " негатив 2. test_data[4000]"
      ]
    },
    {
      "metadata": {
        "id": "DI7YTLKkJsw9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? я был из тех избранных кто попал на ранний предпросмотр. Первое что вы должны знать об этом филье, что он основан на игре Far Cry, для своего 2004 года очень даже годная игра. Во-вторых вы должны знать, что за этим фильмом стоит человек по имени Уве Болл, который берет видео игру ? долгое время ? ? и тд и делает из них фильмы, очень ужасные должен я вам сказать. \n",
        "Я до сих пор помню как увидел ?  \"Мечи короля\" (The King's Swords ) в котором были ужасные ошибки,  к примеру показ 3 разных сцен одновременно причем две из них днем а третья волшебным образом ночью. Но все же вернусь к нашим пирогам. Если вы ожидаете от фильма Far Cry крутого экшена - забудте,  реально дешевые трюки и пластиковые вертолеты ассоциируются со всем кроме зрелищности, если у вас есть ожиданию по сценарию - забудте, даже изначальная легенда игры не спасла фильм от насмешек. Как ни странно игра актеров во многих моментах сделал фильм еще смешнее, только вот в плохом понимании этого слова. Ребята, у меня океазалась прекрасная возможность просмотреть это фильм бесплатно так , что не делайте ошибки и не платите за этот треш. Это фильм достоин только места в худшей сотке.\n"
      ]
    },
    {
      "metadata": {
        "id": "XND-YBb5JoKu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? i was one chosen to see this movie in a sneak preview br br first you should know that this film is based on the video game far cry a for its time r\n",
        "eally good game 2004 second you should know that the of this flick is the great uwe boll this is a man who takes video games ? siege ? ? etc and makes\n",
        " movies out of them very horrible ones br br i still remember when i saw ? the king's swords a ? siege tail there were so horrible mistakes in this fi\n",
        "lm like 3 scenes playing at the same time 2 at day time and one somehow at night br br so lets come to far cry if you expect cool action forget it rea\n",
        "lly cheap tricks and a plastic helicopter are far away from real action if you expect a cool story forget it by the not so bad story of the game this \n",
        "movie is a laugh the actors' playing makes the movie in a lot of moments funny but in a no good way br br i had the chance to see this movie for free \n",
        "so do not do the mistake and pay for this trash its one of my flicks for the bottom 100"
      ]
    },
    {
      "metadata": {
        "id": "5172--WfOzOW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "негатив 3. test_data[24506]"
      ]
    },
    {
      "metadata": {
        "id": "9hpYQE1BOzjH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? я люблю Стивена Сигала, но я не имею понятия о чем был это  фильм. Обычно меня не легко запутать сюжетом, но я так и не вьехал кто был на  чей стороне. Надеюсь его будущие фильмы будут немного лучше, тем более моя жена гооврит, что ему очень к лицу черный цвет ? всеже р"
      ]
    },
    {
      "metadata": {
        "id": "e9klmsoCO5On",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? i like steven seagal but i have not a clue what this movie was about i am not easily lost with movies but i had no idea who was on who's side br br \n",
        "hopefully some of his future movies will be a little better my wife still thinks he looks good in black ? however p"
      ]
    },
    {
      "metadata": {
        "id": "PRaJ8R5hPhHK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "позитив 1. test_data[24603]"
      ]
    },
    {
      "metadata": {
        "id": "UPxXV53LPhMQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Это один из тех фильмов, которые я могу пересматривать раз за разом и не уставать от этого. До сего времени лучшая адаптация комедийной книги. Он мне нравится даже больше чем X-Men. По сути эта картина что то между иксменов и матрицы и она вышла еще и до того как появился герой Весли Снайпса Блэйд. Он просто не ? супергерой . Также  этот фильм не получил рейтниг PG-13. Конечно комиксы по большей части для детей, но мне больше нравится в омих фильмах. Давайте станем пред фактом лицом: если бы мы были в той же ситации стали ли бы ? шторм, так что это реалистичнее чем сам комикс. Адаптация также обладает кое чем, чего не имеют многие, а именно хорошей дракой в конце  между плохим и хорошим парнем. Давайте признаем, что ни один из фильмов про бетмена не имеет крутого боя в конце."
      ]
    },
    {
      "metadata": {
        "id": "O5A8xVLVPieM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? this is one of those movies that i can watch again and again and not get tired of it it is by far one of the best comic book adaptations ever i like\n",
        "d this one even more than x men in fact this movie is sort of a cross between x men and the matrix and it came out before either wesley snipes does a \n",
        "great job with the character of blade he is just not an ? super hero also this movie isn't to get a pg 13 rating sure comic books are for kids mainly \n",
        "but i like a little more in my movies let's face it if we were in these situations we would ? up a storm to so it is more realistic this comic book ad\n",
        "aptation also has something that many don't a good fight in the end between the bad guy and good guy let's face it none of the batman movies had a ver\n",
        "y good ending fight"
      ]
    },
    {
      "metadata": {
        "id": "UdsTgHvJQeik",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "позитив 2. test_data[24605]"
      ]
    },
    {
      "metadata": {
        "id": "yakBc7d7Qel-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? \"Рей\" один из тех фильмов котороый заставлет сделать паузу и задуматься над тем, что ты знаешь  об этом человеке на основе того что читал и слышал о нем и понимаешь, что это даже не сопоставимо. Во время моего первого просмотра я чувствовал себя как ? и забыл что смотрю фильм, а не наблюдаю за происходящим через свое окно. Эта картина настолько неотразима, она засасывает в себя, вовлекает в каждую деталь, ты попадаешь на поездкку по американской горке эмоций и когда все заканчивается ты не хочешь повторять опыт в скором времени, потому что это фильм имеет такой эмоциональный удар, которого многим не хватает. Jamie Foxx заслужил премию Оскар на всех основаниях. Его испольнение великолепно и должно считаться стандартом для любого кто желает сделать чьюто биографию ? в скором времени. Этот фильм настолько же хорошо как и обьект которого он освещает - оба класскика и легенда."
      ]
    },
    {
      "metadata": {
        "id": "y8_dxjm2QeqT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? ray is one of those movies that makes you pause you actually think about what you heard or think about what you read about this man and it doesn't e\n",
        "ven come close during my first viewing of ray i forgot i was watching a movie i felt like a ? tom watching this man's life thru a window this movie is\n",
        " so compelling it drags you in and it involves your every emotion you go thru a emotional roller coaster ride and when it's over you don't want to do \n",
        "it again so soon because it has that kind of emotional punch that other movies are lacking jamie foxx deserved his oscar and quite rightfully so his p\n",
        "erformance is spectacular and it should be held up as the standard for anybody wanting to do a bio ? anytime soon this movie is as good as it's subjec\n",
        "t both deserved the titles classic and legend"
      ]
    },
    {
      "metadata": {
        "id": "WUDWfgdPRbnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "позитив 3. test_data[2100]"
      ]
    },
    {
      "metadata": {
        "id": "7f5tg2r6RbkK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? я был ярым ? фанатом с поздних 70-х и ранних 80-х. Когда  вышел фильм я решил его обязательно посмотреть и после просмотра пошел прямиком в магазин и купил его. Фильм прекрасен, я люблю его анимированные сцены и музыка шикарна. Да, я купил саундтрек на сиди. Рекомендую каждому получить удовольствие от этого фильма. "
      ]
    },
    {
      "metadata": {
        "id": "G7qXmutORbiE",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "? i have been an avid ? fan since the late 70's early 80's when this movie came out it was a must to see it and after seeing it i went right over and \n",
        "bought it the movie is great i love the animated action it brings and the music is great yes i bought the soundtrack on cd a recommended video for eve\n",
        "ryone to watch and enjoy"
      ]
    },
    {
      "metadata": {
        "id": "FbuRmMNkAm7c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "2b606584-5bcc-408e-f335-e12733a38700"
      },
      "cell_type": "code",
      "source": [
        "'''This example demonstrates the use of fasttext for text classification\n",
        "\n",
        "Based on Joulin et al's paper:\n",
        "\n",
        "Bags of Tricks for Efficient Text Classification\n",
        "https://arxiv.org/abs/1607.01759\n",
        "\n",
        "Results on IMDB datasets with uni and bi-gram embeddings:\n",
        "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
        "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "def create_ngram_set(input_list, ngram_value=2):\n",
        "    \"\"\"\n",
        "    Extract a set of n-grams from a list of integers.\n",
        "\n",
        "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
        "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
        "\n",
        "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
        "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
        "    \"\"\"\n",
        "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
        "\n",
        "\n",
        "def add_ngram(sequences, token_indice, ngram_range=2):\n",
        "    \"\"\"\n",
        "    Augment the input list of list (sequences) by appending n-grams values.\n",
        "\n",
        "    Example: adding bi-gram\n",
        "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
        "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
        "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
        "\n",
        "    Example: adding tri-gram\n",
        "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
        "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
        "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
        "    \"\"\"\n",
        "    new_sequences = []\n",
        "    for input_list in sequences:\n",
        "        new_list = input_list[:]\n",
        "        for ngram_value in range(2, ngram_range + 1):\n",
        "            for i in range(len(new_list) - ngram_value + 1):\n",
        "                ngram = tuple(new_list[i:i + ngram_value])\n",
        "                if ngram in token_indice:\n",
        "                    new_list.append(token_indice[ngram])\n",
        "        new_sequences.append(new_list)\n",
        "\n",
        "    return new_sequences\n",
        "\n",
        "# Set parameters:\n",
        "# ngram_range = 2 will add bi-grams features\n",
        "ngram_range = 2\n",
        "max_features = 20000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
        "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
        "\n",
        "if ngram_range > 1:\n",
        "    print('Adding {}-gram features'.format(ngram_range))\n",
        "    # Create set of unique n-gram from the training set.\n",
        "    ngram_set = set()\n",
        "    for input_list in x_train:\n",
        "        for i in range(2, ngram_range + 1):\n",
        "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
        "            ngram_set.update(set_of_ngram)\n",
        "\n",
        "    # Dictionary mapping n-gram token to a unique integer.\n",
        "    # Integer values are greater than max_features in order\n",
        "    # to avoid collision with existing features.\n",
        "    start_index = max_features + 1\n",
        "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
        "    indice_token = {token_indice[k]: k for k in token_indice}\n",
        "\n",
        "    # max_features is the highest integer that could be found in the dataset.\n",
        "    max_features = np.max(list(indice_token.keys())) + 1\n",
        "\n",
        "    # Augmenting x_train and x_test with n-grams features\n",
        "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
        "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
        "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
        "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "\n",
        "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
        "# of all words in the document\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Average train sequence length: 238\n",
            "Average test sequence length: 230\n",
            "Adding 2-gram features\n",
            "Average train sequence length: 476\n",
            "Average test sequence length: 428\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 400)\n",
            "x_test shape: (25000, 400)\n",
            "Build model...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 60s 2ms/step - loss: 0.5862 - acc: 0.7874 - val_loss: 0.4372 - val_acc: 0.8548\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.2877 - acc: 0.9276 - val_loss: 0.3030 - val_acc: 0.8904\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.1432 - acc: 0.9696 - val_loss: 0.2628 - val_acc: 0.8998\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.0775 - acc: 0.9871 - val_loss: 0.2441 - val_acc: 0.9030\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.0437 - acc: 0.9950 - val_loss: 0.2376 - val_acc: 0.9040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f02ad82a358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "metadata": {
        "id": "-xFT5GwkKPeB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "d63546a9-bada-439d-a234-fcbd3c8170c5"
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from keras.datasets import imdb\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Embedding, Reshape, Flatten\n",
        "from keras import optimizers\n",
        "\n",
        "from keras.models import Model\n",
        "from keras.layers import concatenate, Input\n",
        "\n",
        "num_features = 3000\n",
        "sequence_length = 300\n",
        "embedding_dimension = 100\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words = num_features)\n",
        "\n",
        "X_train = pad_sequences(X_train, maxlen = sequence_length)\n",
        "X_test = pad_sequences(X_test, maxlen = sequence_length)\n",
        "\n",
        "filter_sizes = [3, 4, 5]\n",
        "\n",
        "def convolution():\n",
        "    inn = Input(shape = (sequence_length, embedding_dimension, 1))\n",
        "    convolutions = []\n",
        "    # we conduct three convolutions & poolings then concatenate them.\n",
        "    for fs in filter_sizes:\n",
        "        conv = Conv2D(filters = 100, kernel_size = (fs, embedding_dimension), strides = 1, padding = \"valid\")(inn)\n",
        "        nonlinearity = Activation('relu')(conv)\n",
        "        maxpool = MaxPooling2D(pool_size = (sequence_length - fs + 1, 1), padding = \"valid\")(nonlinearity)\n",
        "        convolutions.append(maxpool)\n",
        "        \n",
        "    outt = concatenate(convolutions)\n",
        "    model = Model(inputs = inn, outputs = outt)\n",
        "        \n",
        "    return model\n",
        "\n",
        "def imdb_cnn_3():\n",
        "    \n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = 3000, output_dim = embedding_dimension, input_length = sequence_length))\n",
        "    model.add(Reshape((sequence_length, embedding_dimension, 1), input_shape = (sequence_length, embedding_dimension)))\n",
        "    \n",
        "    # call convolution method defined above\n",
        "    model.add(convolution())\n",
        "    \n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(10))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dropout(0.3))\n",
        "    model.add(Dense(1))\n",
        "    model.add(Activation('sigmoid'))\n",
        "\n",
        "    adam = optimizers.Adam(lr = 0.001)\n",
        "\n",
        "    model.compile(loss='binary_crossentropy', optimizer=adam , metrics=['accuracy'])\n",
        "    \n",
        "    return model\n",
        "\n",
        "model = imdb_cnn_3()\n",
        "\n",
        "history = model.fit(X_train, y_train, batch_size = 50, epochs = 100, validation_split = 0.2, verbose = 0)\n",
        "\n",
        "# plt.plot(history.history['acc'])\n",
        "# plt.plot(history.history['val_acc'])\n",
        "# plt.legend(['training', 'validation'], loc = 'upper left')\n",
        "# plt.show()\n",
        "\n",
        "results = model.evaluate(X_test, y_test)\n",
        "print('Test accuracy: ', results[1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "25000/25000 [==============================] - 7s 295us/step\n",
            "Test accuracy:  0.87504\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j5mlqFCjdcWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "9ace1e1f-0280-430f-d128-897b53a83f92"
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "model_git = Sequential()\n",
        "\n",
        "max_features = 5000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "filters = 250\n",
        "kernel_size = 3\n",
        "hidden_dims = 250\n",
        "epochs = 2\n",
        "\n",
        "\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model_git.add(Embedding(5000,\n",
        "                    50,\n",
        "                    input_length=400))\n",
        "model_git.add(Dropout(0.2))\n",
        "\n",
        "# we add a Convolution1D, which will learn filters\n",
        "# word group filters of size filter_length:\n",
        "model_git.add(Conv1D(250,\n",
        "                 kernel_size,\n",
        "                 padding='valid',\n",
        "                 activation='relu',\n",
        "                 strides=1))\n",
        "# we use max pooling:\n",
        "model_git.add(GlobalMaxPooling1D())\n",
        "\n",
        "# We add a vanilla hidden layer:\n",
        "model_git.add(Dense(250))\n",
        "model_git.add(Dropout(0.2))\n",
        "model_git.add(Activation('relu'))\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model_git.add(Dense(1))\n",
        "model_git.add(Activation('sigmoid'))\n",
        "\n",
        "model_git.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "x_train = sequence.pad_sequences(train_data_imdb, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(test_data_imdb, maxlen=maxlen)\n",
        "\n",
        "model_git.fit(x_train, train_labels_imdb,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, test_labels_imdb))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/2\n",
            "25000/25000 [==============================] - 13s 521us/step - loss: 5.5483 - acc: 0.5672 - val_loss: 6.7034 - val_acc: 0.5692\n",
            "Epoch 2/2\n",
            "25000/25000 [==============================] - 12s 471us/step - loss: 6.9262 - acc: 0.5571 - val_loss: 6.3497 - val_acc: 0.5934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9aa106f668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "tdLYeu4wUJR6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "c7e87afd-133d-4d61-e2c3-7f6973e57753"
      },
      "cell_type": "code",
      "source": [
        "'''This example demonstrates the use of fasttext for text classification\n",
        "\n",
        "Based on Joulin et al's paper:\n",
        "\n",
        "Bags of Tricks for Efficient Text Classification\n",
        "https://arxiv.org/abs/1607.01759\n",
        "\n",
        "Results on IMDB datasets with uni and bi-gram embeddings:\n",
        "    Uni-gram: 0.8813 test accuracy after 5 epochs. 8s/epoch on i7 cpu.\n",
        "    Bi-gram : 0.9056 test accuracy after 5 epochs. 2s/epoch on GTx 980M gpu.\n",
        "'''\n",
        "\n",
        "from __future__ import print_function\n",
        "import numpy as np\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Embedding\n",
        "from keras.layers import GlobalAveragePooling1D\n",
        "from keras.datasets import imdb\n",
        "\n",
        "\n",
        "def create_ngram_set(input_list, ngram_value=2):\n",
        "    \"\"\"\n",
        "    Extract a set of n-grams from a list of integers.\n",
        "\n",
        "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=2)\n",
        "    {(4, 9), (4, 1), (1, 4), (9, 4)}\n",
        "\n",
        "    >>> create_ngram_set([1, 4, 9, 4, 1, 4], ngram_value=3)\n",
        "    [(1, 4, 9), (4, 9, 4), (9, 4, 1), (4, 1, 4)]\n",
        "    \"\"\"\n",
        "    return set(zip(*[input_list[i:] for i in range(ngram_value)]))\n",
        "\n",
        "\n",
        "def add_ngram(sequences, token_indice, ngram_range=2):\n",
        "    \"\"\"\n",
        "    Augment the input list of list (sequences) by appending n-grams values.\n",
        "\n",
        "    Example: adding bi-gram\n",
        "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017}\n",
        "    >>> add_ngram(sequences, token_indice, ngram_range=2)\n",
        "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42]]\n",
        "\n",
        "    Example: adding tri-gram\n",
        "    >>> sequences = [[1, 3, 4, 5], [1, 3, 7, 9, 2]]\n",
        "    >>> token_indice = {(1, 3): 1337, (9, 2): 42, (4, 5): 2017, (7, 9, 2): 2018}\n",
        "    >>> add_ngram(sequences, token_indice, ngram_range=3)\n",
        "    [[1, 3, 4, 5, 1337, 2017], [1, 3, 7, 9, 2, 1337, 42, 2018]]\n",
        "    \"\"\"\n",
        "    new_sequences = []\n",
        "    for input_list in sequences:\n",
        "        new_list = input_list[:]\n",
        "        for ngram_value in range(2, ngram_range + 1):\n",
        "            for i in range(len(new_list) - ngram_value + 1):\n",
        "                ngram = tuple(new_list[i:i + ngram_value])\n",
        "                if ngram in token_indice:\n",
        "                    new_list.append(token_indice[ngram])\n",
        "        new_sequences.append(new_list)\n",
        "\n",
        "    return new_sequences\n",
        "\n",
        "# Set parameters:\n",
        "# ngram_range = 2 will add bi-grams features\n",
        "ngram_range = 2\n",
        "max_features = 20000\n",
        "maxlen = 400\n",
        "batch_size = 32\n",
        "embedding_dims = 50\n",
        "epochs = 5\n",
        "\n",
        "print('Loading data...')\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')\n",
        "print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
        "print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
        "\n",
        "if ngram_range > 1:\n",
        "    print('Adding {}-gram features'.format(ngram_range))\n",
        "    # Create set of unique n-gram from the training set.\n",
        "    ngram_set = set()\n",
        "    for input_list in x_train:\n",
        "        for i in range(2, ngram_range + 1):\n",
        "            set_of_ngram = create_ngram_set(input_list, ngram_value=i)\n",
        "            ngram_set.update(set_of_ngram)\n",
        "\n",
        "    # Dictionary mapping n-gram token to a unique integer.\n",
        "    # Integer values are greater than max_features in order\n",
        "    # to avoid collision with existing features.\n",
        "    start_index = max_features + 1\n",
        "    token_indice = {v: k + start_index for k, v in enumerate(ngram_set)}\n",
        "    indice_token = {token_indice[k]: k for k in token_indice}\n",
        "\n",
        "    # max_features is the highest integer that could be found in the dataset.\n",
        "    max_features = np.max(list(indice_token.keys())) + 1\n",
        "\n",
        "    # Augmenting x_train and x_test with n-grams features\n",
        "    x_train = add_ngram(x_train, token_indice, ngram_range)\n",
        "    x_test = add_ngram(x_test, token_indice, ngram_range)\n",
        "    print('Average train sequence length: {}'.format(np.mean(list(map(len, x_train)), dtype=int)))\n",
        "    print('Average test sequence length: {}'.format(np.mean(list(map(len, x_test)), dtype=int)))\n",
        "\n",
        "print('Pad sequences (samples x time)')\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "print('Build model...')\n",
        "model = Sequential()\n",
        "\n",
        "# we start off with an efficient embedding layer which maps\n",
        "# our vocab indices into embedding_dims dimensions\n",
        "model.add(Embedding(max_features,\n",
        "                    embedding_dims,\n",
        "                    input_length=maxlen))\n",
        "\n",
        "# we add a GlobalAveragePooling1D, which will average the embeddings\n",
        "# of all words in the document\n",
        "model.add(GlobalAveragePooling1D())\n",
        "\n",
        "# We project onto a single unit output layer, and squash it with a sigmoid:\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          validation_data=(x_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading data...\n",
            "25000 train sequences\n",
            "25000 test sequences\n",
            "Average train sequence length: 238\n",
            "Average test sequence length: 230\n",
            "Adding 2-gram features\n",
            "Average train sequence length: 476\n",
            "Average test sequence length: 428\n",
            "Pad sequences (samples x time)\n",
            "x_train shape: (25000, 400)\n",
            "x_test shape: (25000, 400)\n",
            "Build model...\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/5\n",
            "25000/25000 [==============================] - 60s 2ms/step - loss: 0.5862 - acc: 0.7874 - val_loss: 0.4372 - val_acc: 0.8548\n",
            "Epoch 2/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.2877 - acc: 0.9276 - val_loss: 0.3030 - val_acc: 0.8904\n",
            "Epoch 3/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.1432 - acc: 0.9696 - val_loss: 0.2628 - val_acc: 0.8998\n",
            "Epoch 4/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.0775 - acc: 0.9871 - val_loss: 0.2441 - val_acc: 0.9030\n",
            "Epoch 5/5\n",
            "25000/25000 [==============================] - 58s 2ms/step - loss: 0.0437 - acc: 0.9950 - val_loss: 0.2376 - val_acc: 0.9040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f9a70bce6a0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "metadata": {
        "id": "dNwqOkNeYB6X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}